{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Interpretable Machine Learning Notes"
      ],
      "metadata": {
        "id": "-OoK5T6i6y6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Interpretable Machine Learning is an umbrella term that captures the extraction of relevant knowledge from a Machine-Learning model concerning relationships either contained in data or learned by model*"
      ],
      "metadata": {
        "id": "eH9kBcaw8AfK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chapter 2 : Interpretability"
      ],
      "metadata": {
        "id": "4C0cButr7AYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is interpretability?\n",
        "\n",
        "*Interpretability is degreee to which a human can understand the cause of a decision*\n",
        "\n",
        "*Interpretability is the degree to which a human can consistentlu predict the model's result.*\n",
        "\n",
        "What is algorithmic transparency?\n",
        "\n",
        "It is about how the algorithm learns a model from the data and what kind of relationships it can learn. It only requires knowledge of the algorithm and not the data or the learned model.\n",
        "\n",
        "Need for interpretability?\n",
        "\n",
        "  - Human curiosity and learning\n",
        "  - Find meaning in the world\n",
        "  - Goal of science (Extract additional knowledge captured by the model)\n",
        "  - Safety measures\n",
        "  - Detect Bias (There are constraints that are part of the problem formulation but cannot be covered in the loss function or evaluation metric)\n",
        "  - Social Acceptance\n",
        "  - Debug and Audit\n",
        "\n",
        "\n",
        "Desirable featuers of Machine-Learning model:\n",
        "  - Fairness\n",
        "  - Privacy\n",
        "  - Reliability\n",
        "  - Causality\n",
        "  - Trust\n",
        "\n",
        "  \n",
        "  Side Note - Proxy features should be avoided as the input feature can be a proxy for a causal feature but do not actually cause the outcome. (Use only causal features)\n",
        "\n",
        "  \n",
        "  **Intrisinic/Model-Specific Interpretability Method** : Refers to machine learning models that are considered interpretable due to their simple structure.\n",
        "\n",
        "  **Post-hoc/Model Agnostic Interpretability Method** : Refers to the application of interpretation methods after model training."
      ],
      "metadata": {
        "id": "fYvER_4Z8cqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key differntiating factors between interpretability methods:\n",
        "\n",
        "  - Results of Interpretation Method Categories:\n",
        "    1.  Feature Summary Statistic - Feature Importance/Pairwise Feature Interaction Strenght\n",
        "    2.  Feature Summary Viz - Some feature statistic only meaningful when visualized (PDP)\n",
        "    3.  Model Internals(Model Specific) - Interpretation of intrinsically interpretable model (lm weights/decision tree split rule/ visualizations of feature decoders in CNNs)\n",
        "    4. Data Points - Counter factual explanations (useful in text/image data not for tabular data)\n",
        "    5. Intrinsically Interpretable Model - Interpret black-box models by approximating them with an interpretable model and then interpret the interpretable model.\n",
        "\n",
        "\n",
        "  - Model Specific v/s Model Agnostic\n",
        "\n",
        "  - Scope of Interpretability:\n",
        "\n",
        "    1. Algorithmic Transparency (Ideal case - no need for interpretabilty methods)\n",
        "    2. Global/Holistic Model Interpretability - Understanding how model makes decisions based on holistic view of its feature and each of the learned components. Hard to achieve in practice.\n",
        "    3. Global Model Interpretability on a Modular Level\n",
        "    4. Local Interpretability for a Single Prediction - More accurate than global explanations as the other complex behavior of the model might behave more pleasantly for a singe instance.\n",
        "    5. Local Interpretability for a Group of Predictions - Either global model interpretations on modular level treating the group as entire dataset or individual explanations for each member of the group and later aggregate them.\n",
        "\n",
        "  - Evaluation of Interpretability:\n",
        "    1. Application Level Evaluation : Actually verified by experts/SMEs.\n",
        "    2. Human Level Evaluation : Evaluated by laypersons, with the assumption that they will select the best explanation method.\n",
        "    3. Function Level Evaluations : Works best when class of model used has already been evaluated by someone else in a human level evaluation."
      ],
      "metadata": {
        "id": "kiPGzSqncMQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Properties of Explanations:\n",
        "- Expressive Power : Structure/Language of explanations generates\n",
        "- Translucency : Amount of peeking done in the ML model. Highly translucent - rely on more information hence more confidence in the explanations (potentially better).\n",
        "- Portability : Range of ML models it can be used for.\n",
        "- Algorithmic Complexity : Computational complexity."
      ],
      "metadata": {
        "id": "n6CwuiVkGy6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Properties of Individual Explanations:\n",
        "\n",
        "- Accuracy : How well does the explanation predict unseen data?\n",
        "- Fidelity : How well does the explanation approximate the predictions of black-box method?\n",
        "- Consistency : Different models yet similar explanations for a specific task.\n",
        "- Stability : How similar are explanations for similar instances? Same model different instances. Low stability ~ variance, can be caused due to non-deterministic components of the explanation method. (local surrogate/LIME).\n",
        "- Comprehensibillty : Perceptible to humans (highly biased measure)\n",
        "- Certainity\n",
        "- Degree of Importance : How well does it reflect the importance of features/parts?\n",
        "- Novelty\n",
        "- Representativeness : How many instances does an explanation cover?"
      ],
      "metadata": {
        "id": "AfFt8RLnHqhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a good explanation? (Miller's summary on \"good\" explanations):\n",
        "\n",
        "- Explanations are contrastive and usually the most intutive and accepted by humans. When thinking why didn't a drug work you look for a patient with very similar features on whom the drug worked and compare them.\n",
        "When you think of it, as intutive it is; it can be very difficult to automate constrasive explanations.\n",
        "- Explanations are selected : An event can be explained by various causes. The LIME method does a good work of selecting and providing very short explanations even if the world is very complex.\n",
        "- Explanations focus on the abnormal : Always include if any of the features had a rare or abnormal value.\n",
        "- Explanations are truthful : They are truthful in reality meaning they have high fidelity.\n",
        "- Explanantions are consistent with prior beliefs of the explainee : Confirmation bias, difficult to incorporate in the method.\n",
        "- Explanations are general and probable : One cause that can explain many events. Generality can be measured by the feature's support."
      ],
      "metadata": {
        "id": "IS6yaPUAMJQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chapter 3 : Datasets"
      ],
      "metadata": {
        "id": "b1sYC4lA7IgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chapter 4 : Interpretable Models"
      ],
      "metadata": {
        "id": "cLVJ_qq17R59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chapter 5 : Model-Agnostic Methods"
      ],
      "metadata": {
        "id": "ch1fizCn7WQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chapter 6 : Examples"
      ],
      "metadata": {
        "id": "BYCXCPJf7bwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chapter 7 : Global Model-Agnostic Methods"
      ],
      "metadata": {
        "id": "ws1aAGGv7iLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chapter 8 : Local Model-Agnostic Methods"
      ],
      "metadata": {
        "id": "vlc86aJ-7nkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chapter 9 : Neural Network Interpretation"
      ],
      "metadata": {
        "id": "Yaqwp3xf7zwu"
      }
    }
  ]
}